{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Week 1 - Group 5\n",
    "\n",
    "### Noelani Roy, Yihong Qiu, Cosimo Cambi, Craig Perkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import geopy.distance\n",
    "from math import sin, cos, sqrt, atan2, radians, log\n",
    "import imblearn\n",
    "from numpy import mean, where\n",
    "from collections import Counter\n",
    "import qgrid\n",
    "\n",
    "# visual libraries\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../fraudTrain.csv\"\n",
    "filename2 = \"../fraudTest.csv\"\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "df2 = pd.read_csv(filename2)\n",
    "\n",
    "fraud_df = pd.concat([df, df2])\n",
    "\n",
    "# print(fraud_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1852394, 23)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import geopy.distance\n",
    "\n",
    "from math import sin, cos, sqrt, atan2, radians, log\n",
    "\n",
    "def calculate_age(born):\n",
    "    today = date.today()\n",
    "    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    "\n",
    "def calculate_distance(row):\n",
    "    coords_1 = (row['lat'], row['long'])\n",
    "    coords_2 = (row['merch_lat'], row['merch_long'])\n",
    "    return geopy.distance.geodesic(coords_1, coords_2).km\n",
    "\n",
    "# Answer from https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n",
    "# The answers above are based on the Haversine formula, which assumes the earth is a sphere,\n",
    "# which results in errors of up to about 0.5% (according to help(geopy.distance)). \n",
    "# Vincenty distance uses more accurate ellipsoidal models such as WGS-84, and is implemented in geopy. For example,\n",
    "def calculate_distance2(row):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(row['lat'])\n",
    "    lon1 = radians(row['long'])\n",
    "    lat2 = radians(row['merch_lat'])\n",
    "    lon2 = radians(row['merch_long'])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    print(distance)\n",
    "\n",
    "\n",
    "# First derive columns\n",
    "if 'trans_date_trans_time' in fraud_df.columns:\n",
    "    fraud_df['txn_datetime'] =  pd.to_datetime(fraud_df['trans_date_trans_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "if 'dob' in fraud_df.columns:\n",
    "    fraud_df['age'] = [calculate_age(d) for d in pd.to_datetime(fraud_df['dob'], format='%Y-%m-%d')]\n",
    "    \n",
    "if set(['lat', 'long', 'merch_lat', 'merch_long']).issubset(set(fraud_df.columns)):\n",
    "     fraud_df['distance'] = [calculate_distance(row) for _, row in fraud_df.iterrows()]\n",
    "        \n",
    "fraud_df['hour'] = fraud_df['txn_datetime'].dt.hour\n",
    "fraud_df['day'] = fraud_df['txn_datetime'].dt.day\n",
    "fraud_df['month'] = fraud_df['txn_datetime'].dt.month\n",
    "fraud_df['year'] = fraud_df['txn_datetime'].dt.year\n",
    "\n",
    "fraud_df['log_amt'] = [log(n) for n in fraud_df['amt']]\n",
    "    \n",
    "\n",
    "# Drop the columns used to derive new features\n",
    "fraud_df.drop([\n",
    "    'Unnamed: 0',\n",
    "    'trans_num',\n",
    "    'unix_time',\n",
    "    'first',\n",
    "    'last',\n",
    "    'street',\n",
    "    'city',\n",
    "    'state',\n",
    "    'zip',\n",
    "    'dob',\n",
    "    'trans_date_trans_time',\n",
    "    'lat',\n",
    "    'long',\n",
    "    'merch_lat',\n",
    "    'merch_long'\n",
    "], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "\n",
    "cols = list(fraud_df.columns)\n",
    "cols.insert(0, cols.pop(cols.index(\"txn_datetime\")))\n",
    "cols.append(cols.pop(cols.index('is_fraud')))\n",
    "fraud_df = fraud_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.to_csv('fraudFeatures.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"../week2/fraudFeatures.csv\"\n",
    "fraud_df = pd.read_csv(filename1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col = ['category', 'gender']\n",
    "num_col = [\n",
    "    'age',\n",
    "    'distance',\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'hour',\n",
    "    'city_pop'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one hot encodes the categorical columns and create a new variable to hold the nex column headers\n",
    "\n",
    "cat_col_onehotencode = []\n",
    "\n",
    "if len(cat_col) > 0: \n",
    "    cat_onehotencode = pd.get_dummies(fraud_df[cat_col], drop_first=True)\n",
    "    cat_col_onehotencode = list(cat_onehotencode.columns)\n",
    "    fraud_df = pd.concat([fraud_df,cat_onehotencode], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df.drop([\n",
    "    'txn_datetime',\n",
    "    'cc_num',\n",
    "    'job',\n",
    "    'log_amt',\n",
    "    'merchant',\n",
    "    'category',\n",
    "    'gender',\n",
    "], axis=1, errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# fraud_df['cc_num'] = le.fit_transform(fraud_df['cc_num']) \n",
    "# fraud_df['merchant'] = le.fit_transform(fraud_df['merchant']) \n",
    "# fraud_df['category'] = le.fit_transform(fraud_df['category']) \n",
    "# fraud_df['job'] = le.fit_transform(fraud_df['job']) \n",
    "# fraud_df['gender'] = le.fit_transform(fraud_df['gender']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873b36bed1124dcfa695a5eeb5ef203c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': False, 'defaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import qgrid\n",
    "\n",
    "qgrid.show_grid(fraud_df.head(100), grid_options={'forceFitColumns': False, 'defaultColumnWidth': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1842743\n",
       "1       9651\n",
       "Name: is_fraud, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_df[\"is_fraud\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "nm = NearMiss()\n",
    "\n",
    "feature_cols = [col for col in fraud_df.columns if col not in [\"is_fraud\"]]\n",
    "X = fraud_df[feature_cols] # Features\n",
    "y = fraud_df[\"is_fraud\"] # Target variable\n",
    "\n",
    "X_res, y_res = nm.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1842743), (1, 9651)]\n",
      "[(0, 4060), (1, 4381), (2, 3502)]\n",
      "[(0, 4499), (1, 4566), (2, 4413)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "X2, y2 = make_classification(n_samples=5000, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_repeated=0, n_classes=3,\n",
    "                           n_clusters_per_class=1,\n",
    "                           weights=[0.01, 0.05, 0.94],\n",
    "                          class_sep=0.8, random_state=0)\n",
    "print(sorted(Counter(y).items()))\n",
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X2, y2)\n",
    "print(sorted(Counter(y_resampled).items()))\n",
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=0)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X2, y2)\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# os = RandomOverSampler(sampling_strategy = 0.01) # Ratio of minority class / majority class\n",
    "us = RandomUnderSampler(sampling_strategy = 0.25) # Ratio of minority class / majority class\n",
    "\n",
    "feature_cols = [col for col in fraud_df.columns if col not in [\"is_fraud\"]]\n",
    "X = fraud_df[feature_cols] # Features\n",
    "y = fraud_df[\"is_fraud\"] # Target variable\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # ('os', os),\n",
    "    ('us', us)\n",
    "])\n",
    "\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1842743\n",
       "1       9651\n",
       "Name: is_fraud, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(X_res.shape)\n",
    "# print(y_res.shape)\n",
    "\n",
    "fraud_df[\"is_fraud\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    137871\n",
       "1      9651\n",
       "Name: is_fraud, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261303d8582d47f0b1a341d26c7e4e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QgridWidget(grid_options={'fullWidthRows': True, 'syncColumnCellResize': True, 'forceFitColumns': False, 'defaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import qgrid\n",
    "\n",
    "qgrid.show_grid(X_resampled.head(100), grid_options={'forceFitColumns': False, 'defaultColumnWidth': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(y_test)/2)\n",
    "\n",
    "inputX = X_train.to_numpy()\n",
    "inputY = y_train.to_numpy()\n",
    "inputX_valid = X_test.to_numpy()[:split]\n",
    "inputY_valid = y_test.to_numpy()[:split]\n",
    "inputX_test = X_test.to_numpy()[split:]\n",
    "inputY_test = y_test.to_numpy()[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of input nodes.\n",
    "input_nodes = 37\n",
    "\n",
    "# Multiplier maintains a fixed ratio of nodes between each layer.\n",
    "mulitplier = 1.5 \n",
    "\n",
    "# Number of nodes in each hidden layer\n",
    "hidden_nodes1 = 18\n",
    "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
    "hidden_nodes3 = round(hidden_nodes2 * mulitplier)\n",
    "\n",
    "# Percent of nodes to keep during dropout.\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "pkeep = tf.compat.v1.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.compat.v1.placeholder(tf.float32, [None, input_nodes])\n",
    "\n",
    "# layer 1\n",
    "W1 = tf.Variable(tf.compat.v1.truncated_normal([input_nodes, hidden_nodes1], stddev = 0.15))\n",
    "b1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "y1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "\n",
    "# layer 2\n",
    "W2 = tf.Variable(tf.compat.v1.truncated_normal([hidden_nodes1, hidden_nodes2], stddev = 0.15))\n",
    "b2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "# layer 3\n",
    "W3 = tf.Variable(tf.compat.v1.truncated_normal([hidden_nodes2, hidden_nodes3], stddev = 0.15)) \n",
    "b3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "y3 = tf.nn.sigmoid(tf.matmul(y2, W3) + b3)\n",
    "y3 = tf.nn.dropout(y3, pkeep)\n",
    "\n",
    "# layer 4\n",
    "W4 = tf.Variable(tf.compat.v1.truncated_normal([hidden_nodes3, 2], stddev = 0.15)) \n",
    "b4 = tf.Variable(tf.zeros([2]))\n",
    "y4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)\n",
    "\n",
    "# output\n",
    "y = y4\n",
    "y_ = tf.compat.v1.placeholder(tf.float32, [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 5 # should be 2000, it will timeout when uploading\n",
    "training_dropout = 0.9\n",
    "display_step = 1 # 10 \n",
    "n_samples = y_train.shape[0]\n",
    "batch_size = 2048\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2b8bf016a413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# We will optimize our model via AdamOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Correct prediction if the most likely value (Fraud or Normal) from softmax equals the target value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Northeastern/EAI6000/eai6000_group5/env/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     grads_and_vars = self._compute_gradients(\n\u001b[0m\u001b[1;32m    375\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Northeastern/EAI6000/eai6000_group5/env/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# Cost function: Cross Entropy\n",
    "cost = - tf.reduce_sum(y_ * tf.compat.v1.log(y))\n",
    "\n",
    "# We will optimize our model via AdamOptimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate).minimize(cost, var_list=[W1, b1])\n",
    "\n",
    "# Correct prediction if the most likely value (Fraud or Normal) from softmax equals the target value.\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               5888      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 137,729\n",
      "Trainable params: 137,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(\n",
    "            256, activation=\"relu\", input_shape=(X_train.shape[-1],)\n",
    "        ),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.697814591576378e-05\n",
      "0.00014847809948032666\n"
     ]
    }
   ],
   "source": [
    "counts = y_train.value_counts()\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "print(weight_for_0)\n",
    "print(weight_for_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 1s 54ms/step - loss: 2218.4282 - false_negatives_1: 4819.0000 - false_positives_1: 6318.0000 - true_negatives_1: 20725.0000 - true_positives_1: 1916.0000 - precision_1: 0.2327 - recall_1: 0.2845 - val_loss: 10.1849 - val_false_negatives_1: 1992.0000 - val_false_positives_1: 71.0000 - val_true_negatives_1: 11490.0000 - val_true_positives_1: 924.0000 - val_precision_1: 0.9286 - val_recall_1: 0.3169\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 22.4393 - false_negatives_1: 3779.0000 - false_positives_1: 4424.0000 - true_negatives_1: 22619.0000 - true_positives_1: 2956.0000 - precision_1: 0.4005 - recall_1: 0.4389 - val_loss: 3.9500 - val_false_negatives_1: 1751.0000 - val_false_positives_1: 139.0000 - val_true_negatives_1: 11422.0000 - val_true_positives_1: 1165.0000 - val_precision_1: 0.8934 - val_recall_1: 0.3995\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 4.8220 - false_negatives_1: 3631.0000 - false_positives_1: 2965.0000 - true_negatives_1: 24078.0000 - true_positives_1: 3104.0000 - precision_1: 0.5115 - recall_1: 0.4609 - val_loss: 0.7186 - val_false_negatives_1: 2125.0000 - val_false_positives_1: 89.0000 - val_true_negatives_1: 11472.0000 - val_true_positives_1: 791.0000 - val_precision_1: 0.8989 - val_recall_1: 0.2713\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 23ms/step - loss: 2.0302 - false_negatives_1: 4619.0000 - false_positives_1: 1424.0000 - true_negatives_1: 25619.0000 - true_positives_1: 2116.0000 - precision_1: 0.5977 - recall_1: 0.3142 - val_loss: 0.5403 - val_false_negatives_1: 2034.0000 - val_false_positives_1: 108.0000 - val_true_negatives_1: 11453.0000 - val_true_positives_1: 882.0000 - val_precision_1: 0.8909 - val_recall_1: 0.3025\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 27ms/step - loss: 0.6643 - false_negatives_1: 4532.0000 - false_positives_1: 792.0000 - true_negatives_1: 26251.0000 - true_positives_1: 2203.0000 - precision_1: 0.7356 - recall_1: 0.3271 - val_loss: 0.4199 - val_false_negatives_1: 1899.0000 - val_false_positives_1: 123.0000 - val_true_negatives_1: 11438.0000 - val_true_positives_1: 1017.0000 - val_precision_1: 0.8921 - val_recall_1: 0.3488\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.4194 - false_negatives_1: 4454.0000 - false_positives_1: 339.0000 - true_negatives_1: 26704.0000 - true_positives_1: 2281.0000 - precision_1: 0.8706 - recall_1: 0.3387 - val_loss: 0.3880 - val_false_negatives_1: 1748.0000 - val_false_positives_1: 131.0000 - val_true_negatives_1: 11430.0000 - val_true_positives_1: 1168.0000 - val_precision_1: 0.8992 - val_recall_1: 0.4005\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 26ms/step - loss: 0.3885 - false_negatives_1: 4286.0000 - false_positives_1: 320.0000 - true_negatives_1: 26723.0000 - true_positives_1: 2449.0000 - precision_1: 0.8844 - recall_1: 0.3636 - val_loss: 0.3733 - val_false_negatives_1: 1792.0000 - val_false_positives_1: 129.0000 - val_true_negatives_1: 11432.0000 - val_true_positives_1: 1124.0000 - val_precision_1: 0.8970 - val_recall_1: 0.3855\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 0.3806 - false_negatives_1: 4214.0000 - false_positives_1: 299.0000 - true_negatives_1: 26744.0000 - true_positives_1: 2521.0000 - precision_1: 0.8940 - recall_1: 0.3743 - val_loss: 0.3774 - val_false_negatives_1: 1823.0000 - val_false_positives_1: 124.0000 - val_true_negatives_1: 11437.0000 - val_true_positives_1: 1093.0000 - val_precision_1: 0.8981 - val_recall_1: 0.3748\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 25ms/step - loss: 2.4996 - false_negatives_1: 4080.0000 - false_positives_1: 346.0000 - true_negatives_1: 26697.0000 - true_positives_1: 2655.0000 - precision_1: 0.8847 - recall_1: 0.3942 - val_loss: 0.3624 - val_false_negatives_1: 1751.0000 - val_false_positives_1: 135.0000 - val_true_negatives_1: 11426.0000 - val_true_positives_1: 1165.0000 - val_precision_1: 0.8962 - val_recall_1: 0.3995\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 24ms/step - loss: 0.3719 - false_negatives_1: 3845.0000 - false_positives_1: 388.0000 - true_negatives_1: 26655.0000 - true_positives_1: 2890.0000 - precision_1: 0.8816 - recall_1: 0.4291 - val_loss: 0.3485 - val_false_negatives_1: 1435.0000 - val_false_positives_1: 164.0000 - val_true_negatives_1: 11397.0000 - val_true_positives_1: 1481.0000 - val_precision_1: 0.9003 - val_recall_1: 0.5079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1378eb130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "metrics = [\n",
    "    keras.metrics.FalseNegatives(),\n",
    "    keras.metrics.FalsePositives(),\n",
    "    keras.metrics.TrueNegatives(),\n",
    "    keras.metrics.TruePositives(),\n",
    "    keras.metrics.Precision(),\n",
    "    keras.metrics.Recall(),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n",
    ")\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"fraud_model_at_epoch_{epoch}.h5\")]\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=2048,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test, y_test),\n",
    "    # class_weight=class_weight,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
